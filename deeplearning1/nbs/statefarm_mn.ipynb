{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "import keras.layers.convolutional as convolutional\n",
    "from keras.models import Sequential\n",
    "import json\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "import numpy\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "from shutil import copyfile, rmtree\n",
    "from IPython.display import FileLink\n",
    "# reload(vgg_statefarm)\n",
    "# from vgg_statefarm import Vgg16\n",
    "import vgg_statefarm as v\n",
    "from vgg_statefarm import BcolzArrayIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'vgg_statefarm' from 'vgg_statefarm.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(vgg_statefarm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The best approach would be to create a bunch of models, each time holding out one driver, and then average the validation across all of them (you could also average the predictions across all of them, like the ensembling we did in MNIST last week) But I wouldn't bother with that until you had done all the experimenting you wanted to do, since it adds a lot of time to each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# v.mn_valid(local=True)\n",
    "# v.rm_sample_set(local=True)\n",
    "# v.mn_sample(local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "data_dir = current_dir + '/data/statefarm/'\n",
    "# data_dir = current_dir + '/data/statefarm/sample/'\n",
    "results_path=data_dir + 'results/'\n",
    "train_path=data_dir + 'train/'\n",
    "valid_path=data_dir + 'valid/'\n",
    "test_path=data_dir + 'test/'\n",
    "model_path=data_dir + 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peel VGG back to Convolutions, Data Aug, Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = v.Vgg16()\n",
    "vgg.ft(num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15671 images belonging to 10 classes.\n",
      "Found 6753 images belonging to 10 classes.\n",
      "Found 15671 images belonging to 10 classes.\n",
      "Found 6753 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "aug_trn_batches = get_batches(train_path, gen_t, batch_size=batch_size)\n",
    "aug_val_batches = get_batches(valid_path, batch_size=batch_size*2, shuffle=False)\n",
    "trn_batches = get_batches(train_path, batch_size=batch_size)\n",
    "val_batches = get_batches(valid_path, batch_size=batch_size*2, shuffle=False)\n",
    "test_batches = get_batches(test_path, batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aug_trn_features = vgg.model.predict_generator(aug_trn_batches, aug_trn_batches.nb_sample*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'aug_trn_features.dat', aug_trn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = vgg.model.predict_generator(trn_batches, trn_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'trn_features.dat', trn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_features = vgg.model.predict_generator(val_batches, val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'val_features.dat', val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_features = vgg.model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'test_features.dat', test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_trn_features = np.concatenate([aug_trn_features, trn_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'all_trn_features.dat', all_trn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aug_trn_features = bcolz.open(model_path+'aug_trn_features.dat')\n",
    "trn_features = bcolz.open(model_path+'trn_features.dat')\n",
    "val_features = bcolz.open(model_path+'val_features.dat')\n",
    "test_features = bcolz.open(model_path+'test_features.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aug_trn_features = load_array(model_path+'aug_trn_features.dat')\n",
    "# trn_features = load_array(model_path+'trn_features.dat')\n",
    "# all_trn_features = load_array(model_path+'all_trn_features.dat')\n",
    "# val_features = load_array(model_path+'val_features.dat')\n",
    "# test_features = load_array(model_path+'test_features.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_labels = onehot(trn_batches.classes)\n",
    "val_labels = onehot(val_batches.classes)\n",
    "aug_trn_labels = np.concatenate([trn_labels]*5)\n",
    "all_trn_labels = np.concatenate([trn_labels]*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aug_trn_batches_bcolz = BcolzArrayIterator(aug_trn_features, aug_trn_labels, batch_size=aug_trn_features.chunklen * 10, shuffle=True)\n",
    "trn_batches_bcolz = BcolzArrayIterator(trn_features, trn_labels, batch_size=trn_features.chunklen * 10, shuffle=True)\n",
    "val_batches_bcolz = BcolzArrayIterator(val_features, val_labels, batch_size=val_features.chunklen * 20, shuffle=False)\n",
    "# test_batches = BcolzArrayIterator(test_features, test_labels, batch_size=trn_features.chunklen * 20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aug_model = vgg.get_fc_model(.2, aug_trn_batches)\n",
    "aug_model.compile(Adam(lr=1e-10), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "no_of_epoch=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6753"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_batches.nb_sample\n",
    "trn_batches_bcolz.N\n",
    "val_batches_bcolz.N\n",
    "val_batches.nb_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "15671/15671 [==============================] - 12s - loss: 3.0491 - acc: 0.1027 - val_loss: 2.7064 - val_acc: 0.1060\n",
      "Epoch 2/2\n",
      "15671/15671 [==============================] - 12s - loss: 3.0411 - acc: 0.1022 - val_loss: 2.7270 - val_acc: 0.1056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f43d50a4bd0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aug_model.fit(aug_trn_features, aug_trn_labels, batch_size=batch_size, nb_epoch=no_of_epoch, \n",
    "#              validation_data=(val_features, val_labels))\n",
    "\n",
    "aug_model.fit_generator(trn_batches_bcolz, samples_per_epoch=trn_batches_bcolz.N, nb_epoch=no_of_epoch, validation_data=val_batches_bcolz, nb_val_samples=val_batches_bcolz.N)\n",
    "\n",
    "# aug_model.fit(all_trn_features, all_trn_labels, batch_size=batch_size, nb_epoch=no_of_epoch, \n",
    "#              validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aug_model.optimizer.lr=0.01\n",
    "no_of_epoch=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aug_model.fit(all_trn_features, all_trn_labels, batch_size=batch_size, nb_epoch=no_of_epoch, \n",
    "             validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.0001\n",
    "no_of_epoch=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aug_model.fit(all_trn_features, all_trn_labels, batch_size=batch_size, nb_epoch=no_of_epoch, \n",
    "             validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aug_model.save_weights(model_path+'aug_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = load_array(results_path+'test_features.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_preds = aug_model.predict(val_features, batch_size=batch_size*2)\n",
    "preds = aug_model.predict(test_features, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.metrics.categorical_crossentropy(val_labels, do_clip(val_preds, 0.93)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = do_clip(preds,0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm_name = results_path+'/subm.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(batches.class_indices, key=batches.class_indices.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'img', [a[4:] for a in test_filenames])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(subm_name, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FileLink(subm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proc_wgts(layer, prev_p, new_p):\n",
    "    scal = (1-prev_p)/(1-new_p)\n",
    "    return [o*scal for o in layer.get_weights()]\n",
    "\n",
    "opt = RMSprop(lr=0.00001, rho=0.7)\n",
    "def get_fc_model(p, batches):\n",
    "    model = Sequential([\n",
    "        MaxPooling2D(input_shape=vgg.model.layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(p),\n",
    "        BatchNormalization(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(p),\n",
    "        BatchNormalization(),\n",
    "        Dense(batches.nb_class, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    for l in model.layers: \n",
    "        if type(l)==Dense: l.set_weights(proc_wgts(l, 0.3, 0.6))\n",
    "\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc_model = get_fc_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "no_of_epochs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1500 samples, validate on 1000 samples\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 184s - loss: 4.2066 - acc: 0.1107 - val_loss: 4.6126 - val_acc: 0.0950\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 176s - loss: 2.2797 - acc: 0.3360 - val_loss: 4.3633 - val_acc: 0.0860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x121caa790>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.fit(trn_features, trn_labels, nb_epoch=no_of_epochs, batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1500 samples, validate on 1000 samples\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 182s - loss: 1.1495 - acc: 0.6080 - val_loss: 4.3291 - val_acc: 0.0960\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 176s - loss: 0.5279 - acc: 0.8360 - val_loss: 4.3173 - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x121cb40d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.optimizer.lr = 0.00001\n",
    "fc_model.fit(trn_features, trn_labels, nb_epoch=no_of_epochs, batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_of_epochs=4\n",
    "fc_model.optimizer.lr = 0.001\n",
    "fc_model.fit(trn_features, trn_labels, nb_epoch=no_of_epochs, batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model.save_weights(model_path+'sf_sample.h5')\n",
    "fc_model.load_weights(model_path+'sf_sample.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv1(batches):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    return model\n",
    "\n",
    "def conv2(batches):\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(128,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(train_path, gen_t, batch_size=batch_size)\n",
    "val_batches = get_batches(valid_path, batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15671 images belonging to 10 classes.\n",
      "Found 6753 images belonging to 10 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, filenames, test_filenames) = get_classes(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "trn = get_data(train_path)\n",
    "val = get_data(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_array(data_dir+'results/val.dat', val)\n",
    "save_array(data_dir+'results/trn.dat', trn)\n",
    "# val = load_array(path+'results/val.dat')\n",
    "# trn = load_array(path+'results/trn.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 33s - loss: 2.2838 - acc: 0.2693 - val_loss: 2.1581 - val_acc: 0.3030\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 33s - loss: 1.7052 - acc: 0.4447 - val_loss: 1.6523 - val_acc: 0.4790\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 34s - loss: 1.5338 - acc: 0.4887 - val_loss: 1.6045 - val_acc: 0.4600\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 32s - loss: 1.4162 - acc: 0.5093 - val_loss: 1.4913 - val_acc: 0.5350\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 33s - loss: 1.3188 - acc: 0.5720 - val_loss: 1.4043 - val_acc: 0.5080\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 33s - loss: 1.2640 - acc: 0.5867 - val_loss: 1.2721 - val_acc: 0.5950\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 33s - loss: 1.1632 - acc: 0.6200 - val_loss: 1.4224 - val_acc: 0.5550\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 32s - loss: 1.0810 - acc: 0.6553 - val_loss: 1.3246 - val_acc: 0.5780\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 33s - loss: 1.0340 - acc: 0.6740 - val_loss: 1.2650 - val_acc: 0.6030\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 32s - loss: 1.0163 - acc: 0.6667 - val_loss: 1.0990 - val_acc: 0.6510\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 32s - loss: 1.0364 - acc: 0.6600 - val_loss: 1.4614 - val_acc: 0.5510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f430bea5910>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1500/1500 [==============================] - 34s - loss: 0.9446 - acc: 0.7000 - val_loss: 1.0697 - val_acc: 0.6660\n",
      "Epoch 2/25\n",
      "1500/1500 [==============================] - 32s - loss: 0.9120 - acc: 0.7287 - val_loss: 1.1718 - val_acc: 0.6330\n",
      "Epoch 3/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.9165 - acc: 0.7113 - val_loss: 1.1122 - val_acc: 0.6380\n",
      "Epoch 4/25\n",
      "1500/1500 [==============================] - 32s - loss: 0.8506 - acc: 0.7373 - val_loss: 1.1121 - val_acc: 0.6100\n",
      "Epoch 5/25\n",
      "1500/1500 [==============================] - 32s - loss: 0.7845 - acc: 0.7553 - val_loss: 0.9349 - val_acc: 0.6820\n",
      "Epoch 6/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.7507 - acc: 0.7640 - val_loss: 1.0202 - val_acc: 0.6560\n",
      "Epoch 7/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.7962 - acc: 0.7540 - val_loss: 0.9893 - val_acc: 0.6760\n",
      "Epoch 8/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.7403 - acc: 0.7647 - val_loss: 0.9372 - val_acc: 0.6910\n",
      "Epoch 9/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.6969 - acc: 0.7853 - val_loss: 0.9677 - val_acc: 0.7000\n",
      "Epoch 10/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.6644 - acc: 0.8020 - val_loss: 1.0080 - val_acc: 0.6870\n",
      "Epoch 11/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.6495 - acc: 0.8133 - val_loss: 1.0306 - val_acc: 0.6660\n",
      "Epoch 12/25\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 0.6729 - acc: 0.7942"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-14e2804574fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(batches, batches.nb_sample, nb_epoch=25, validation_data=val_batches, \n\u001b[0;32m----> 2\u001b[0;31m                  nb_val_samples=val_batches.nb_sample)\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1469\u001b[0m                         val_outs = self.evaluate_generator(validation_data,\n\u001b[1;32m   1470\u001b[0m                                                            \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m                                                            max_q_size=max_q_size)\n\u001b[0m\u001b[1;32m   1472\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m                         \u001b[0;31m# no need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1534\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1536\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=25, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
