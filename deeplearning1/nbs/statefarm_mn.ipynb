{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils import *\n",
    "import keras.layers.convolutional as convolutional\n",
    "from keras.models import Sequential\n",
    "import json\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "import numpy\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "from shutil import copyfile, rmtree\n",
    "import allnew_vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best approach would be to create a bunch of models, each time holding out one driver, and then average the validation across all of them (you could also average the predictions across all of them, like the ensembling we did in MNIST last week) But I wouldn't bother with that until you had done all the experimenting you wanted to do, since it adds a lot of time to each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mn_valid(validation_frac=.3, local=False):\n",
    "    if local:\n",
    "        path = '/Users/matthew.negrin/deeplearning/courses/deeplearning1/nbs/data/statefarm/'\n",
    "    else:\n",
    "        path = '/home/ubuntu/matt/courses/deeplearning1/nbs/data/statefarm/'\n",
    "    direct_list = ['valid/'+x[0].split('/')[-1] for x in os.walk(path+'train/') if x[0].split('/')[-1] != '']\n",
    "    direct_list.insert(0, \"valid\")\n",
    "    for directory in direct_list:\n",
    "        if not os.path.exists(path+directory):\n",
    "            os.makedirs(path+directory)\n",
    "    dil = pd.read_csv(path + 'driver_imgs_list.csv')\n",
    "    valid_subjects = dil.groupby(['subject']).size().sample(frac=validation_frac).keys().tolist()\n",
    "    valid_frame = dil[dil['subject'].isin(valid_subjects)]\n",
    "    for (subject, classname, img) in valid_frame.values:\n",
    "        source = '{}train/{}/{}'.format(path, classname, img)\n",
    "        target = source.replace('train', 'valid')\n",
    "        print('mv {} {}'.format(source, target))\n",
    "        os.rename(source, target)\n",
    "\n",
    "def rm_sample_set():\n",
    "    path = '/home/ubuntu/matt/courses/deeplearning1/nbs/data/statefarm/'\n",
    "    rmtree(path+'sample/')\n",
    "        \n",
    "def mn_sample():\n",
    "    path = '/home/ubuntu/matt/courses/deeplearning1/nbs/data/statefarm/'\n",
    "    os.makedirs(path+'sample')\n",
    "    os.makedirs(path+'sample/train')\n",
    "    os.makedirs(path+'sample/valid')\n",
    "    os.makedirs(path+'sample/models')\n",
    "    os.makedirs(path+'sample/results')\n",
    "    direct_list = [x[0].split('/')[-1] for x in os.walk(path+'train/') if x[0].split('/')[-1] != '']\n",
    "    for d in direct_list:\n",
    "        os.makedirs(path+'sample/train/'+d)\n",
    "        os.mkdir(path+'sample/valid/'+d)\n",
    "    tg = glob(path+'train/c?/*.jpg')\n",
    "    shuf = np.random.permutation(tg)\n",
    "    for i in range(1500): copyfile(shuf[i],path+'sample/train/' + shuf[i].split('/')[-2]+'/'+shuf[i].split('/')[-1])\n",
    "    vg = glob(path+'valid/c?/*.jpg')\n",
    "    vshuf = np.random.permutation(vg)\n",
    "    for i in range(1000): copyfile(vshuf[i],path+'sample/valid/' + vshuf[i].split('/')[-2]+'/'+vshuf[i].split('/')[-1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mn_valid()\n",
    "# mn_sample()\n",
    "# rm_sample_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "# data_dir = current_dir + '/data/statefarm/' #sample/\n",
    "data_dir = current_dir + '/data/statefarm/sample/'\n",
    "results_path=data_dir + 'results/'\n",
    "train_path=data_dir + 'train/'\n",
    "valid_path=data_dir + 'valid/'\n",
    "test_path=data_dir + 'test/'\n",
    "model_path=data_dir + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv1(batches):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    return model\n",
    "\n",
    "def conv2(batches):\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(128,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(train_path, gen_t, batch_size=batch_size)\n",
    "val_batches = get_batches(valid_path, batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15671 images belonging to 10 classes.\n",
      "Found 6753 images belonging to 10 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, filenames, test_filenames) = get_classes(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "trn = get_data(train_path)\n",
    "val = get_data(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_array(data_dir+'results/val.dat', val)\n",
    "save_array(data_dir+'results/trn.dat', trn)\n",
    "# val = load_array(path+'results/val.dat')\n",
    "# trn = load_array(path+'results/trn.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 33s - loss: 2.2838 - acc: 0.2693 - val_loss: 2.1581 - val_acc: 0.3030\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 33s - loss: 1.7052 - acc: 0.4447 - val_loss: 1.6523 - val_acc: 0.4790\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 34s - loss: 1.5338 - acc: 0.4887 - val_loss: 1.6045 - val_acc: 0.4600\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 32s - loss: 1.4162 - acc: 0.5093 - val_loss: 1.4913 - val_acc: 0.5350\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 33s - loss: 1.3188 - acc: 0.5720 - val_loss: 1.4043 - val_acc: 0.5080\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 33s - loss: 1.2640 - acc: 0.5867 - val_loss: 1.2721 - val_acc: 0.5950\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 33s - loss: 1.1632 - acc: 0.6200 - val_loss: 1.4224 - val_acc: 0.5550\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 32s - loss: 1.0810 - acc: 0.6553 - val_loss: 1.3246 - val_acc: 0.5780\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 33s - loss: 1.0340 - acc: 0.6740 - val_loss: 1.2650 - val_acc: 0.6030\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 32s - loss: 1.0163 - acc: 0.6667 - val_loss: 1.0990 - val_acc: 0.6510\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 32s - loss: 1.0364 - acc: 0.6600 - val_loss: 1.4614 - val_acc: 0.5510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f430bea5910>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1500/1500 [==============================] - 34s - loss: 0.9446 - acc: 0.7000 - val_loss: 1.0697 - val_acc: 0.6660\n",
      "Epoch 2/25\n",
      "1500/1500 [==============================] - 32s - loss: 0.9120 - acc: 0.7287 - val_loss: 1.1718 - val_acc: 0.6330\n",
      "Epoch 3/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.9165 - acc: 0.7113 - val_loss: 1.1122 - val_acc: 0.6380\n",
      "Epoch 4/25\n",
      "1500/1500 [==============================] - 32s - loss: 0.8506 - acc: 0.7373 - val_loss: 1.1121 - val_acc: 0.6100\n",
      "Epoch 5/25\n",
      "1500/1500 [==============================] - 32s - loss: 0.7845 - acc: 0.7553 - val_loss: 0.9349 - val_acc: 0.6820\n",
      "Epoch 6/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.7507 - acc: 0.7640 - val_loss: 1.0202 - val_acc: 0.6560\n",
      "Epoch 7/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.7962 - acc: 0.7540 - val_loss: 0.9893 - val_acc: 0.6760\n",
      "Epoch 8/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.7403 - acc: 0.7647 - val_loss: 0.9372 - val_acc: 0.6910\n",
      "Epoch 9/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.6969 - acc: 0.7853 - val_loss: 0.9677 - val_acc: 0.7000\n",
      "Epoch 10/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.6644 - acc: 0.8020 - val_loss: 1.0080 - val_acc: 0.6870\n",
      "Epoch 11/25\n",
      "1500/1500 [==============================] - 33s - loss: 0.6495 - acc: 0.8133 - val_loss: 1.0306 - val_acc: 0.6660\n",
      "Epoch 12/25\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 0.6729 - acc: 0.7942"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-14e2804574fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(batches, batches.nb_sample, nb_epoch=25, validation_data=val_batches, \n\u001b[0;32m----> 2\u001b[0;31m                  nb_val_samples=val_batches.nb_sample)\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1469\u001b[0m                         val_outs = self.evaluate_generator(validation_data,\n\u001b[1;32m   1470\u001b[0m                                                            \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m                                                            max_q_size=max_q_size)\n\u001b[0m\u001b[1;32m   1472\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m                         \u001b[0;31m# no need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1534\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1536\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=25, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
